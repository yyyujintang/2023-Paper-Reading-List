# (NeurIPS22 Outstanding Paper) Beyond neural scaling laws: beating power law scaling via data pruning

[TOC]



## Abstract

1. 从神经网络训练的广泛观察来看，错误率是随着训练集大小，模型大小，而呈现幂函数变化的。
2. 但如果要通过提升训练集大小，提升复兴复杂度来得到模型表现的提升，需要消耗相当多的算力和能量。
3. 这篇文章聚焦于数据集的大小，展示了如何从理论上打破了幂律缩放，甚至有可能将其降低到指数缩放，如果可以得到高效的数据剪枝度量指标的的话。
4. 用剪枝后的数据集大小经验地测试了这种改进的缩放预测，并且在CIFAR-10、SVHN和ImageNet上训练的ResNets实践中确实观察到比幂律缩放更好。
5. 鉴于高质量剪枝度量指标的重要性，在ImageNet上做了第一个大规模的benchmark实验，对十个不同的数据剪枝指标进行了实验比对。
6. 实验发现，大多数现有的高性能指标在ImageNet上的伸缩性很差，而最好的指标是计算密集型的，并且需要为每张图像打标签。
7. 因此，我们开发了一种新的简单、廉价且可扩展的自监督剪枝指标，该指标显示出与最佳监督指标相当的性能。

## Contribution

1. 利用统计力学，我们开发了一种新的数据剪枝分析理论，用于感知器学习的学生-教师设置，其中示例根据其教师间隔(margin)进行修剪，大(小)间隔对应于简单(困难)示例。我们的理论在数量上与数值实验相匹配，并揭示了两个惊人的预测:
   (a)最优剪枝策略随初始数据量的变化而变化;对于丰富(稀缺)的初始数据，应该只保留难(简单)的示例。(b)如果选择一个增加的帕累托最优剪枝分数作为初始数据集大小的函数，那么对于剪枝数据集大小，指数缩放是可能的。
2. 我们表明，这两个从理论中得出的惊人的预测在更一般的情况下也适用于实践。事实上，我们通过经验证明了在SVHN、CIFAR-10和ImageNet上从头训练的ResNets以及在CIFAR-10上微调的Vision transformer的数据集大小方面，误差的指数缩放特征。
3. 基于为数据修剪找到高质量指标的重要性，我们在ImageNet上对10种不同的数据修剪指标进行了大规模的基准测试研究，发现除了最密集的计算指标外，大多数指标表现不佳。
4. 我们利用自监督学习(SSL)开发了一种新的、廉价的无监督数据修剪指标，与以前的指标不同，它不需要标签。我们表明，这种无监督指标的表现与需要标签和更多计算的最佳监督修剪指标相当。这一结果为利用预训练的基础模型在标记新数据集之前修剪新数据集打开了一扇令人兴奋的大门。

## Background and related work

### Pruning Metrics

- [ ] ==EL2N scores==：在很短的时间内(大约10个epoch)训练了小的集合(大约10个)网络，并为每个训练示例计算了误差向量的平均L2范数(EL2N评分)。通过只保留最难且误差最大的示例来进行数据修剪。
- [ ] ==Forgetting scores and classification margins==：开发了一种遗忘评分，用来衡量每个例子的遗忘程度。直观上，遗忘得分低(高)的例子可以被认为是简单(难)的例子。
- [ ] ==Memorization and influence==：为每个例子定义了一个记忆分数，对应于当这个例子出现在训练集中时，相对于它不出现时，预测该例子正确标签的概率增加了多少。考虑了一个影响分数，量化了向训练集中添加特定示例，增加测试示例正确类标签的概率的程度。直观地说，较低的记忆和影响力分数对应于与其余数据冗余的简单示例，而较高的分数对应于必须单独学习的困难示例。